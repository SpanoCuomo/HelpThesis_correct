#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import time
import argparse
from typing import List, Dict, Any

# ------------------------ Prompt template ------------------------

SYSTEM_PROMPT = """Sei un giurista didattico. Per ogni domanda fornita, aggiungi una spiegazione HTML molto dettagliata nel campo "explanation".
Lo stile DEVE seguire questa struttura a paragrafi (in italiano):
1) <p>‚úÖ <strong>Risposta corretta: ...</strong></p>
2) <p>Paragrafo introduttivo che spiega il contesto e i concetti chiave.</p>
3) <p>Riferimento normativo preciso (articolo di legge/c.p.c./c.c., ecc.) e cosa dispone.</p>
4) <p>üëâ Ratio/funzione della regola: perch√© esiste, cosa tutela, come si applica.</p>
5) <p>üí° <strong>Esempio pratico:</strong> esempio concreto e realistico.</p>

Importante:
- Mantieni intatti "id", "q", "options", "correct" (non modificarli).
- Compila SOLO il nuovo campo "explanation" come stringa HTML (nessun markdown).
- La spiegazione deve essere originale, non ripetitiva, e coerente con la risposta corretta.
- Non inventare norme se non necessarie: cita articoli reali dove possibile (es.: art. 147 c.p.c.).
- Restituisci SOLO un JSON array di oggetti con chiavi: id, explanation.
"""

# ‚ö†Ô∏è RADDOPPIA LE GRAFFE per evitare KeyError con .format(...)
USER_PROMPT_TEMPLATE = """Completa con "explanation" i seguenti quesiti.
Restituisci SOLO un JSON array, in cui ogni elemento √® {{\"id\": <id>, \"explanation\": \"<html>\"}}
DOMANDE:
{payload}
"""

# ------------------------ LLM adapter (OpenAI v1) ------------------------

def call_llm(messages: List[Dict[str, str]], model: str, temperature: float = 0.2) -> str:
    """
    Usa openai>=1.0.0:
      pip install --upgrade openai
    """
    try:
        from openai import OpenAI
    except Exception as e:
        raise RuntimeError("Libreria 'openai' non installata. Esegui: pip install openai") from e

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("Variabile d'ambiente OPENAI_API_KEY mancante.")

    client = OpenAI()
    resp = client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=messages
    )
    return resp.choices[0].message.content

# ------------------------ Utility ------------------------

def chunk_list(seq, n):
    return [seq[i:i+n] for i in range(0, len(seq), n)]

def normalize_item(item: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "id": int(item["id"]),
        "q": item["q"],
        "options": {
            "A": item.get("A") or item.get("options", {}).get("A", ""),
            "B": item.get("B") or item.get("options", {}).get("B", ""),
            "C": item.get("C") or item.get("options", {}).get("C", ""),
        },
        "correct": item.get("correct", ""),
    }

def php_escape(s: str) -> str:
    return s.replace("\\", "\\\\").replace('"', '\\"')

def to_php_array(questions: List[Dict[str, Any]]) -> str:
    rows = []
    for q in questions:
        row = []
        row.append('  [')
        row.append(f'    "id" => {q["id"]},')
        row.append(f'    "q"  => "{php_escape(q["q"])}",')
        row.append(f'    "options" => ["A" => "{php_escape(q["options"]["A"])}", "B" => "{php_escape(q["options"]["B"])}", "C" => "{php_escape(q["options"]["C"])}"],')
        row.append(f'    "correct" => "{php_escape(q["correct"])}",')
        if q.get("explanation"):
            row.append(f'    "explanation" => "{php_escape(q["explanation"])}"')
        else:
            row[-1] = row[-1].rstrip(",")
        row.append('  ]')
        rows.append("\n".join(row))
    return "<?php\n$questions = [\n" + ",\n".join(rows) + "\n];\n?>\n"

def save_php(out_path: str, questions_by_id: Dict[int, Dict[str, Any]]):
    ordered = [questions_by_id[i] for i in sorted(questions_by_id)]
    php_text = to_php_array(ordered)
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(php_text)

# ------------------------ Core ------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in", dest="in_path", required=True, help="File JSON input (quiz_ag800sc.json)")
    ap.add_argument("--out", dest="out_path", required=True, help="File PHP output ($questions array)")
    ap.add_argument("--model", default="gpt-5", help="Nome modello (es. gpt-5)")
    ap.add_argument("--chunk", type=int, default=20, help="Dimensione blocchi (default 20)")
    ap.add_argument("--temperature", type=float, default=0.2, help="Temperature LLM")
    ap.add_argument("--sleep", type=float, default=0.7, help="Pausa fra i batch per rate limiting")
    ap.add_argument("--save-every", action="store_true", help="Scrive/aggiorna il file PHP dopo ogni batch")
    ap.add_argument("--min-words", type=int, default=0, help="Se >0, chiede spiegazioni di almeno N parole")
    args = ap.parse_args()

    with open(args.in_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    questions = [normalize_item(it) for it in raw]
    questions_by_id = {q["id"]: q for q in questions}
    all_ids = [q["id"] for q in questions]
    batches = chunk_list(all_ids, args.chunk)

    # opzionale: rafforza profondit√†
    sys_prompt = SYSTEM_PROMPT
    if args.min_words and args.min_words > 0:
        sys_prompt += f"\n- Ogni spiegazione deve contenere almeno {args.min_words} parole."

    for idx, batch_ids in enumerate(batches, 1):
        batch_items = [questions_by_id[i] for i in batch_ids]
        payload = json.dumps(batch_items, ensure_ascii=False, indent=2)
        user_prompt = USER_PROMPT_TEMPLATE.format(payload=payload)

        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ]

        print(f"[Batch {idx}/{len(batches)}] Invio {len(batch_items)} domande al modello‚Ä¶")

        # retry semplice con backoff
        backoff = 1.0
        for attempt in range(5):
            try:
                text = call_llm(messages, model=args.model, temperature=args.temperature).strip()
                break
            except Exception as e:
                print(f"  Tentativo {attempt+1} fallito: {e}")
                time.sleep(backoff)
                backoff *= 1.6
        else:
            raise RuntimeError("Troppe failure consecutive verso l'API.")

        try:
            resp = json.loads(text)
            if not isinstance(resp, list):
                raise ValueError("La risposta LLM non √® un array JSON.")
        except Exception as e:
            # log diagnostico
            preview = text[:800].replace("\n", " ")
            raise RuntimeError(f"Parsing JSON fallito. Risposta (inizio): {preview}") from e

        for obj in resp:
            qid = int(obj["id"])
            exp = obj.get("explanation", "").strip()
            if qid in questions_by_id and exp:
                questions_by_id[qid]["explanation"] = exp

        if args.save_every:
            save_php(args.out_path, questions_by_id)
            print(f"  ‚úì Aggiornato: {args.out_path}")

        time.sleep(args.sleep)

    # salvataggio finale
    save_php(args.out_path, questions_by_id)
    print(f"‚úÖ File PHP generato: {args.out_path}")

if __name__ == "__main__":
    main()
